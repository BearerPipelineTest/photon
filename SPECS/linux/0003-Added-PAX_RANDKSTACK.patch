From 1f81333ed0e2847e859268bf084dce7de3199e22 Mon Sep 17 00:00:00 2001
From: Alexey Makhalov <amakhalov@vmware.com>
Date: Sat, 4 Feb 2017 04:15:14 +0000
Subject: [PATCH] Added PAX_RANDKSTACK

---
 arch/x86/entry/entry_64.S    | 14 ++++++++++++++
 arch/x86/kernel/process_64.c | 21 +++++++++++++++++++++
 security/Kconfig             | 14 ++++++++++++++
 3 files changed, 49 insertions(+)

diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index c864245f..25d67670 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -56,6 +56,16 @@ ENTRY(native_usergs_sysret64)
 ENDPROC(native_usergs_sysret64)
 #endif /* CONFIG_PARAVIRT */
 
+.macro pax_rand_kstack
+#ifdef CONFIG_PAX_RANDKSTACK
+	pushq   %rax
+	pushq   %r11
+	call    pax_randomize_kstack
+	popq    %r11
+	popq    %rax
+#endif
+.endm
+
 .macro TRACE_IRQS_IRETQ
 #ifdef CONFIG_TRACE_IRQFLAGS
 	btl	$9, EFLAGS(%rsp)		/* interrupts off? */
@@ -201,6 +211,8 @@ GLOBAL(entry_SYSCALL_64_after_swapgs)
 	movq	%rsp, %rdi
 	call	do_syscall_64		/* returns with IRQs disabled */
 
+	pax_rand_kstack
+
 	RESTORE_EXTRA_REGS
 	TRACE_IRQS_IRETQ		/* we're about to change IF */
 
@@ -364,6 +376,7 @@ ENTRY(ret_from_fork)
 2:
 	movq	%rsp, %rdi
 	call	syscall_return_slowpath	/* returns with IRQs disabled */
+	pax_rand_kstack
 	TRACE_IRQS_ON			/* user mode is traced as IRQS on */
 	SWITCH_USER_CR3
 	SWAPGS
@@ -480,6 +493,7 @@ ret_from_intr:
 GLOBAL(retint_user)
 	mov	%rsp,%rdi
 	call	prepare_exit_to_usermode
+	pax_rand_kstack
 	TRACE_IRQS_IRETQ
 	SWITCH_USER_CR3
 	SWAPGS
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 6d6c15cd..9a17acae 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -264,7 +264,13 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 	struct inactive_task_frame *frame;
 	struct task_struct *me = current;
 
+#ifdef CONFIG_PAX_RANDKSTACK
+	/* -16 to start from prev page (c000 -> bff0)
+           to avoid stack overflow after randomizarion */
+	p->thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE - 16;
+#else
 	p->thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE;
+#endif
 	childregs = task_pt_regs(p);
 	fork_frame = container_of(childregs, struct fork_frame, regs);
 	frame = &fork_frame->frame;
@@ -653,3 +659,18 @@ unsigned long KSTK_ESP(struct task_struct *task)
 {
 	return task_pt_regs(task)->sp;
 }
+
+#ifdef CONFIG_PAX_RANDKSTACK
+void pax_randomize_kstack(void)
+{
+	struct thread_struct *thread = &current->thread;
+	unsigned long time;
+
+	if (!randomize_va_space)
+		return;
+
+	time = rdtsc() & 0xFUL;
+	thread->sp0 ^= (time << 4);
+	load_sp0(&per_cpu(cpu_tss, smp_processor_id()), thread);
+}
+#endif
diff --git a/security/Kconfig b/security/Kconfig
index d5641c41..d649311c 100644
--- a/security/Kconfig
+++ b/security/Kconfig
@@ -99,6 +99,20 @@ config PAX_RAP
 	  i.e., gcc 4.5 or newer.  You may need to install the supporting
 	  headers explicitly in addition to the normal gcc package.
 
+config PAX_RANDKSTACK
+	bool "Randomize kernel stack base"
+	depends on X86_TSC && X86
+	help
+	  By saying Y here the kernel will randomize every task's kernel
+	  stack on every system call.  This will not only force an attacker
+	  to guess it but also prevent him from making use of possible
+	  leaked information about it.
+
+	  Since the kernel stack is a rather scarce resource, randomization
+	  may cause unexpected stack overflows, therefore you should very
+	  carefully test your system.  Note that once enabled in the kernel
+	  configuration, this feature cannot be disabled on a per file basis.
+
 endif
 
 source security/keys/Kconfig
-- 
2.23.1

